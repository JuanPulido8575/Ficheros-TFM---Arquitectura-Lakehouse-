{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3d2a03-a2d3-4f0c-a0e0-13c67992c5dd",
   "metadata": {},
   "source": [
    "## Instalación del paquete confluent-kafka para interactuar con Kafka desde Python sin usar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca65e5-67d3-4e8d-80d7-94018039a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3285e99-2088-4f8c-b0a0-7f5878b42d23",
   "metadata": {},
   "source": [
    "## Funciones auxiliares para crear y borrar el topic revisiones (completa el código con el nombre de tu cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75874c93-653c-4ca7-a796-7e28f8bc44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "import socket\n",
    "\n",
    "def crear_topic(admin_client: AdminClient, topic_name: str):\n",
    "    \"\"\"\n",
    "    Crea el topic llamado topic_name con una sola partición y factor replicación 1.\n",
    "    No devuelve nada pero mostrará un mensaje indicando si ha ido bien o no.\n",
    "    \n",
    "    :param admin_client: objeto AdminClient con el cliente de Kafka ya configurado\n",
    "    :param topic_name: string con el nombre del topic que deseamos crear\n",
    "    \"\"\"\n",
    "    topic = NewTopic(topic_name, num_partitions=1, replication_factor=1)\n",
    "    fs = admin_client.create_topics([topic])\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()\n",
    "            print(f\"Topic {topic_name} creado\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fallo al crear el topic {topic_name}: {e}\")\n",
    "\n",
    "def borrar_topic(admin_client: AdminClient, topic_name: str):\n",
    "    \"\"\"\n",
    "    Borra un topic existente llamado topic_name que debe existir.\n",
    "    No devuelve nada pero mostrará un mensaje indicando si ha ido bien o no.\n",
    "    \n",
    "    :param admin_client: objeto AdminClient con el cliente de Kafka ya configurado\n",
    "    :param topic_name: string con el nombre del topic que deseamos borrar\n",
    "    \"\"\"\n",
    "    fs = admin_client.delete_topics([topic_name])\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()\n",
    "            print(f\"Topic {topic_name} borrado\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fallo al borrar el topic {topic_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e388f9-d9f7-4994-810e-f91d8fc1471b",
   "metadata": {},
   "source": [
    "## Cliente para interactuar con Kafka mediante el paquete confluent-kafka, sin usar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa5d16-bc3c-481b-9533-05d502355f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_cluster = \"jppg\"    # COMPLETA ESTA LÍNEA\n",
    "\n",
    "# DESCOMENTA ESTAS LÍNEAS PARA CREAR EL CLIENTE CON EL NOMBRE DEL CLUSTER\n",
    "\n",
    "conf = {\"bootstrap.servers\": f\"{nombre_cluster}-w-0:9092,{nombre_cluster}-w-1:9092\",\n",
    "         \"sasl.mechanism\": \"PLAIN\",\n",
    "        \"client.id\": socket.gethostname()}\n",
    "\n",
    "producer = Producer(conf)\n",
    "admin_client = AdminClient(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffdc2b-799b-4a15-b3fa-0e4e5075cf52",
   "metadata": {},
   "source": [
    "### Funciones para borrar el topic, si algo hubiera ido mal y queremos repetir, y para crearlo tras haberlo borrado (o la primera vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb3b18-0a58-4ed9-a2ba-7939d5b813c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrar_topic(admin_client, \"Call Event\")\n",
    "crear_topic(admin_client, \"Call Event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705e19d-ca8a-4c60-a5e6-d63d6d5b71df",
   "metadata": {},
   "source": [
    "## Función para insertar datos en Kafka, en lotes de 10.000 cada 2 segundos, para dar tiempo a visualizar el resultado cambiante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324557b5-be4b-4c30-87f5-b75a126735e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=1)\n",
    "\n",
    "def insertar_kafka(producer: Producer, ruta_csv: str):\n",
    "    \"\"\"\n",
    "    Inserta en Kafka los mensajes de las filas del fichero CSV en ruta_csv,\n",
    "    escogiendo solo las columnas dest, dep_time y arr_delay. Cada fila es\n",
    "    un mensaje formateado como JSON. \n",
    "    \"\"\"\n",
    "    # Función que ignoraremos ya que la arquitectura de Data Lakehouse solo utiliza almacenamiento del lago de datos\n",
    "    if not os.path.isfile(\"calls.csv\"):\n",
    "        print(\"Trayendo fichero flights.csv desde HDFS al bucket de DBFS de Databricks Comunity Edition ...\")\n",
    "        os.system(f\"hdfs dfs -copyToLocal -f {ruta_csv} .\")\n",
    "    \n",
    "    print(\"Leyendo datos del CSV ...\")\n",
    "    calls_df = pd.read_csv(\"calls.csv\")\n",
    "    \n",
    "    # Formateamos los mensajes como cadenas de texto con estructura de JSON con todos los campos a escribir\n",
    "    calls_dict = calls_df.transpose().to_dict()\n",
    "    mensajes = [str(calls_dict[i]).replace(\"'\", \"\\\"\") for i in range(len(calls_dict))]\n",
    "    \n",
    "    print(\"Insertando mensajes en el topic Call Event ...\")\n",
    "    \n",
    "    def insertar_bucle():\n",
    "        import time\n",
    "        for i, msj in enumerate(mensajes):\n",
    "            producer.produce(\"Call Event\", value=msj)\n",
    "            if i % 10000 == 0:\n",
    "                print(f\"Se ha insertado el mensaje {i} con valor {msj}\")\n",
    "                producer.flush()\n",
    "                time.sleep(5)\n",
    "    \n",
    "    # Ejecutamos de manera asíncrona todo el bucle de inserciones, para simultáneamente poder \n",
    "    # ejecutar la otra celda en la que hacemos show de la tabla de salida donde escribe Spark Structured Streaming\n",
    "    resultado = executor.submit(insertar_bucle)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529e870-20cd-4569-909c-d2984b65796f",
   "metadata": {},
   "source": [
    "## Lectura de mensajes del topic creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900ceb7-7465-46fa-b3b6-f7a23fe9f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto hace más eficientes las transformaciones wide con volúmenes pequeños de datos\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# Reemplaza por el código correcto siguiendo las indicaciones anteriores\n",
    "callsStreamingDF = spark.readStream\\\n",
    "                        .format(\"kafka\")\\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"jppg-w-0:9092,jppg-w-1:9092\")\\\n",
    "                        .option(\"subscribe\", \"retrasos\")\\\n",
    "                        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1670919-960a-4467-a221-ef9eae01575c",
   "metadata": {},
   "source": [
    "Muestra por pantalla el esquema del DataFrame resultante de la lectura con `printSchema()`. Verás que todas estas columnas son creadas automáticamente por Spark cuando leemos de Kafka. De ellas, la que nos interesa es `value` que contiene propiamente el mensaje de Kafka, en formato datos binarios. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75db5cf-327c-408a-993a-a8b7dabb6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Inicia Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bronze_By_Candidate\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1) Definimos el super‑schema con todas las columnas\n",
    "esquema = StructType([\n",
    "    StructField(\"call_date\",           StringType(), nullable=True),\n",
    "    StructField(\"phone_number_dialed\", StringType(), nullable=True),\n",
    "    StructField(\"status\",              StringType(), nullable=True),\n",
    "    StructField(\"user\",                StringType(), nullable=True),\n",
    "    StructField(\"full_name\",           StringType(), nullable=True),\n",
    "    StructField(\"campaign_id\",         StringType(), nullable=True),\n",
    "    StructField(\"vendor_lead_code\",    StringType(), nullable=True),\n",
    "    StructField(\"source_id\",           StringType(), nullable=True),\n",
    "    StructField(\"list_id\",             StringType(), nullable=True),\n",
    "    StructField(\"gmt_offset\",          DoubleType(), nullable=True),\n",
    "    StructField(\"alt_phone\",           StringType(), nullable=True),\n",
    "    StructField(\"security_phrase\",     StringType(), nullable=True),\n",
    "    StructField(\"comments\",            StringType(), nullable=True),\n",
    "    StructField(\"length_in_sec\",       DoubleType(), nullable=True),\n",
    "    StructField(\"user_group\",          StringType(), nullable=True),\n",
    "    StructField(\"alt_dial\",            StringType(), nullable=True),\n",
    "    StructField(\"list_name\",           StringType(), nullable=True),  # clave de partición con el nombre del candidato de la campaña\n",
    "    StructField(\"status_name\",         StringType(), nullable=True),\n",
    "    StructField(\"custom_fields\",       StringType(), nullable=True),\n",
    "    StructField(\"yard_sign\",           StringType(), nullable=True),\n",
    "    StructField(\"top_issue\",           StringType(), nullable=True),\n",
    "    StructField(\"campaign\",            StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# 2) Leemos el stream original (p. ej. desde Kafka) en callsStreamingDF...\n",
    "# callsStreamingDF = spark.readStream.format(\"kafka\")...load()\n",
    "\n",
    "# 3) Parseamos el JSON usando el esquema\n",
    "parsedDF = (\n",
    "    callsStreamingDF\n",
    "      .select(F.col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "      .withColumn(\"parsed\", F.from_json(\"json_str\", esquema))\n",
    "      .select(\n",
    "          F.col(\"parsed.call_date\"),\n",
    "          F.col(\"parsed.phone_number_dialed\"),\n",
    "          F.col(\"parsed.status\"),\n",
    "          F.col(\"parsed.user\"),\n",
    "          F.col(\"parsed.full_name\"),\n",
    "          F.col(\"parsed.campaign_id\"),\n",
    "          F.col(\"parsed.vendor_lead_code\"),\n",
    "          F.col(\"parsed.source_id\"),\n",
    "          F.col(\"parsed.list_id\"),\n",
    "          F.col(\"parsed.gmt_offset\"),\n",
    "          F.col(\"parsed.alt_phone\"),\n",
    "          F.col(\"parsed.security_phrase\"),\n",
    "          F.col(\"parsed.comments\"),\n",
    "          F.col(\"parsed.length_in_sec\"),\n",
    "          F.col(\"parsed.user_group\"),\n",
    "          F.col(\"parsed.alt_dial\"),\n",
    "          F.col(\"parsed.list_name\"),        # nombre del candidato\n",
    "          F.col(\"parsed.status_name\"),\n",
    "          F.col(\"parsed.custom_fields\"),\n",
    "          F.col(\"parsed.yard_sign\"),\n",
    "          F.col(\"parsed.top_issue\"),\n",
    "          F.col(\"parsed.campaign\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# 4) Función para escribir cada microbatch por candidato\n",
    "bronze_root = \"/mnt/Data Call Center/Bronze\"\n",
    "\n",
    "def write_by_candidate(batch_df, batch_id):\n",
    "    batch_df.persist()\n",
    "    # obtenemos candidatos únicos\n",
    "    candidates = [row.list_name for row in batch_df.select(\"list_name\").distinct().collect()]\n",
    "    for cand in candidates:\n",
    "        out_path = f\"{bronze_root}/{cand}/Calls\"\n",
    "        (batch_df\n",
    "            .filter(F.col(\"list_name\") == cand)\n",
    "            .write\n",
    "            .mode(\"append\")\n",
    "            .parquet(out_path)      # o .format(\"delta\").save(out_path)\n",
    "        )\n",
    "    batch_df.unpersist()\n",
    "\n",
    "# 5) Arrancamos el streaming con foreachBatch\n",
    "(\n",
    "    parsedDF.writeStream\n",
    "        .foreachBatch(write_by_candidate)\n",
    "        .option(\"checkpointLocation\", \"/mnt/bronze/_checkpoints/by_candidate\")\n",
    "        .start()\n",
    "        .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025d826-9fcf-46cb-acf5-dcfcd0c94e3b",
   "metadata": {},
   "source": [
    "## Inserción en Kafka + lectura desde Kafka con Spark y agregación en tiempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9d094-deb8-419f-95c2-5a9c697e72c5",
   "metadata": {},
   "source": [
    "Se crea una tabla apuntando a los ficheros de la capa Bronze en Calls para verificar que efectivamente los mensajes se están escribiendo \n",
    "correctamente en la capa Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b7ddc-5175-44b8-b391-a4df448b3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuerda que en el proceso de escritura en el tópico, renombrarás previamente las columnas vacías del Dataframe para evitar lidiar con \n",
    "# columnas vacías en el super-esquema que se proporcionará al Streaming Dataframe. \n",
    "# Recuerda el ejemplo:\n",
    "#import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame({\n",
    " #   'A': [1,2],\n",
    "  #  'B': [3,4]\n",
    "#})\n",
    "\n",
    "# Diccionario con una clave que no está en df ('C')\n",
    "#mapping = {'A': 'alpha', 'C': 'gamma'}\n",
    "\n",
    "#df2 = df.rename(columns=mapping)\n",
    "#print(df2.columns)\n",
    "# Index(['alpha', 'B'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292ae79-a461-44bc-8274-6e23b24b2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "ruta_calls_dbfs = \"dbfs://jppg/datos/calls.csv\"   # ruta del fichero calls.csv en el sistema de ficheros dsitribuido de Databricks CE\n",
    "resultado = insertar_kafka(producer, ruta_calls_dbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aecef5-e4c2-41c4-8391-bb87d08311d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Registro_Bronze_Dinamico\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1) Creamos la base de datos si no existe\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS call_center_db\")\n",
    "\n",
    "# 2) Creamos la tabla externa apuntando recursivamente al root de Bronze\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS call_center_db.bronze_calls_dynamic (\n",
    "    call_date            STRING,\n",
    "    phone_number_dialed  STRING,\n",
    "    status               STRING,\n",
    "    user                 STRING,\n",
    "    full_name            STRING,\n",
    "    campaign_id          STRING,\n",
    "    vendor_lead_code     STRING,\n",
    "    source_id            STRING,\n",
    "    list_id              STRING,\n",
    "    gmt_offset           DOUBLE,\n",
    "    alt_phone            STRING,\n",
    "    security_phrase      STRING,\n",
    "    comments             STRING,\n",
    "    length_in_sec        DOUBLE,\n",
    "    user_group           STRING,\n",
    "    alt_dial             STRING,\n",
    "    list_name            STRING,\n",
    "    status_name          STRING,\n",
    "    custom_fields        STRING,\n",
    "    yard_sign            STRING,\n",
    "    top_issue            STRING,\n",
    "    campaign             STRING\n",
    "  )\n",
    "  USING parquet\n",
    "  OPTIONS (\n",
    "  path '/mnt/Data Call Center/Bronze/*/Calls/*.parquet',\n",
    "  recursiveFileLookup 'false'\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b629f-459e-4e27-b71a-abe0d9afde23",
   "metadata": {},
   "source": [
    "Llamamos la tabla SQL y la mostramos cada 7 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c14b3-201e-4ed4-a107-4c8f25adb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for i in range(15):\n",
    "    time.sleep(7)\n",
    "    # Re‑ejecutar la consulta\n",
    "    df = spark.sql(\"SELECT * FROM call_center_db.bronze_calls_dynamic\")\n",
    "    df.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
